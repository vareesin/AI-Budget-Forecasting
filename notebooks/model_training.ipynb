{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 1. Prepare Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "\n",
    "# Load the 2023 profit data\n",
    "file_path_2023 = 'Profits_2023.xlsx'\n",
    "months_2023 = ['Jan23', 'Feb23', 'mar23', 'Apr23', 'may23', 'Jun23', 'Jul23', 'Aug23', 'Sep23', 'Oct23', 'Nov23', 'Dec23']\n",
    "\n",
    "# Extract profit data for 2023 from the \"กำไรหลังAudit/Wo\" column\n",
    "df_2023 = pd.concat([pd.read_excel(file_path_2023, sheet_name=month)[['Store Name', 'กำไรหลังAudit/Wo']].rename(columns={'กำไรหลังAudit/Wo': month})\n",
    "                     for month in months_2023], axis=1)\n",
    "\n",
    "# Drop duplicate columns and reset index to avoid duplicate 'Store Name'\n",
    "df_2023 = df_2023.loc[:, ~df_2023.columns.duplicated()]\n",
    "\n",
    "# Load the 2024 profit data\n",
    "file_path_2024 = 'Profits_2024.xlsx'\n",
    "\n",
    "# Extract January and February 2024 profit data\n",
    "df_2024_jan_feb = pd.concat([pd.read_excel(file_path_2024, sheet_name=month)[['Store Name', 'กำไรหลังAudit/Wo']].rename(columns={'กำไรหลังAudit/Wo': month})\n",
    "                             for month in ['มค', 'กพ']], axis=1)\n",
    "\n",
    "# Drop duplicate columns\n",
    "df_2024_jan_feb = df_2024_jan_feb.loc[:, ~df_2024_jan_feb.columns.duplicated()]\n",
    "\n",
    "# Load March to June 2024 data with correct header rows\n",
    "df_2024_mar = pd.read_excel(file_path_2024, sheet_name='มีค', header=1)[['Store Name', 'กำไรหลังAudit/Wo']].rename(columns={'กำไรหลังAudit/Wo': 'มีค'})\n",
    "df_2024_apr = pd.read_excel(file_path_2024, sheet_name='เมย', header=1)[['Store Name', 'กำไรระดับสาขารวมโบนัส+ปันส่วนจาก 7-11']].rename(columns={'กำไรระดับสาขารวมโบนัส+ปันส่วนจาก 7-11': 'เมย'})\n",
    "df_2024_may = pd.read_excel(file_path_2024, sheet_name='พค', header=1)[['Store Name', 'กำไรระดับสาขารวมโบนัส+ปันส่วนจาก 7-11']].rename(columns={'กำไรระดับสาขารวมโบนัส+ปันส่วนจาก 7-11': 'พค'})\n",
    "df_2024_jun = pd.read_excel(file_path_2024, sheet_name='มิย', header=2)[['Store Name', 'กำไรระดับสาขารวมโบนัส+ปันส่วนจาก 7-11']].rename(columns={'กำไรระดับสาขารวมโบนัส+ปันส่วนจาก 7-11': 'มิย'})\n",
    "\n",
    "# Combine March to June data\n",
    "df_2024_mar_jun = pd.merge(df_2024_mar, df_2024_apr, on='Store Name', how='outer')\n",
    "df_2024_mar_jun = pd.merge(df_2024_mar_jun, df_2024_may, on='Store Name', how='outer')\n",
    "df_2024_mar_jun = pd.merge(df_2024_mar_jun, df_2024_jun, on='Store Name', how='outer')\n",
    "\n",
    "# Drop duplicate columns in March to June data\n",
    "df_2024_mar_jun = df_2024_mar_jun.loc[:, ~df_2024_mar_jun.columns.duplicated()]\n",
    "\n",
    "# Combine the January to February data with the March to June data\n",
    "df_2024_combined = pd.merge(df_2024_jan_feb, df_2024_mar_jun, on='Store Name', how='outer')\n",
    "\n",
    "# Drop duplicate columns in the combined 2024 data\n",
    "df_2024_combined = df_2024_combined.loc[:, ~df_2024_combined.columns.duplicated()]\n",
    "\n",
    "# Now, merge the 2023 profit data with the 2024 combined data\n",
    "combined_profits = pd.merge(df_2023, df_2024_combined, on='Store Name', how='outer')\n",
    "\n",
    "# Ensure no duplicate columns are present\n",
    "combined_profits = combined_profits.loc[:, ~combined_profits.columns.duplicated()]\n",
    "\n",
    "# Update the month names in the dataframe to follow the format \"Jan2023\", \"Feb2023\", etc.\n",
    "month_mapping = {\n",
    "    'Jan23': 'Jan2023', 'Feb23': 'Feb2023', 'mar23': 'Mar2023', 'Apr23': 'Apr2023',\n",
    "    'may23': 'May2023', 'Jun23': 'Jun2023', 'Jul23': 'Jul2023', 'Aug23': 'Aug2023',\n",
    "    'Sep23': 'Sep2023', 'Oct23': 'Oct2023', 'Nov23': 'Nov2023', 'Dec23': 'Dec2023',\n",
    "    'มค': 'Jan2024', 'กพ': 'Feb2024', 'มีค': 'Mar2024', 'เมย': 'Apr2024',\n",
    "    'พค': 'May2024', 'มิย': 'Jun2024'\n",
    "}\n",
    "\n",
    "# Rename the columns to reflect the new format\n",
    "combined_profits = combined_profits.rename(columns=month_mapping)\n",
    "\n",
    "# Remove rows where 'Store Name' is NaN or the row has missing data\n",
    "combined_profits = combined_profits.dropna(subset=['Store Name']).reset_index(drop=True)\n",
    "\n",
    "# Re-export the cleaned data to an Excel file\n",
    "output_path = 'Cleaned_Combined_Profit_Data_2023_2024.xlsx'\n",
    "combined_profits.to_excel(output_path, index=False)\n",
    "\n",
    "output_path\n",
    "\n",
    "# You can save or further process the combined_profits dataframe\n",
    "combined_profits\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Visualize Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import matplotlib.font_manager as fm\n",
    "\n",
    "# Load the cleaned dataset with Thai encoding\n",
    "file_path = 'Cleaned_Combined_Profit_Data_2023_2024.xlsx'\n",
    "data = pd.read_excel(file_path)\n",
    "\n",
    "# Ensure Thai fonts are available\n",
    "# You can specify the path to a Thai font if necessary, e.g., 'THSarabunNew.ttf'\n",
    "font_path = 'THSarabunNew Bold.ttf'  # Example for Linux\n",
    "# For Windows: font_path = 'C:/Windows/Fonts/THSarabunNew.ttf'\n",
    "thai_font = fm.FontProperties(fname=font_path)\n",
    "\n",
    "# Extract store names and monthly profit columns\n",
    "store_names = data['Store Name']\n",
    "profit_columns = data.columns[1:]  # All columns from Jan2023 to Jun2024\n",
    "\n",
    "# Plot profits for each store\n",
    "plt.figure(figsize=(12, 8))\n",
    "\n",
    "# Loop through each store and plot its profits\n",
    "for i, store in enumerate(store_names):\n",
    "    profits = data.iloc[i, 1:].values  # Get profits for this store across all months\n",
    "    plt.plot(profit_columns, profits, label=store)\n",
    "\n",
    "# Customize the plot\n",
    "plt.title('Monthly Profits for All Stores (Jan2023 to Jun2024)', fontsize=14, fontproperties=thai_font)\n",
    "plt.xlabel('Month', fontsize=12, fontproperties=thai_font)\n",
    "plt.ylabel('Profit', fontsize=12, fontproperties=thai_font)\n",
    "plt.xticks(rotation=45, fontproperties=thai_font)\n",
    "plt.legend(loc='best', bbox_to_anchor=(1, 1), prop=thai_font)  # Puts legend outside the plot\n",
    "plt.tight_layout()\n",
    "\n",
    "# Show the plot\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 2. Finding best model for individual stores"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "from sklearn.preprocessing import MinMaxScaler\n",
    "from sklearn.ensemble import RandomForestRegressor\n",
    "from xgboost import XGBRegressor\n",
    "from sklearn.metrics import mean_absolute_percentage_error as mape\n",
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.layers import LSTM, GRU, Conv1D, MaxPooling1D, Dense, Dropout\n",
    "from tensorflow.keras.optimizers import Adam\n",
    "from sklearn.model_selection import GridSearchCV, TimeSeriesSplit\n",
    "from scipy import stats\n",
    "from prophet import Prophet\n",
    "import os\n",
    "\n",
    "# Load the cleaned dataset\n",
    "file_path = 'Cleaned_Combined_Profit_Data_2023_2024.xlsx'\n",
    "data = pd.read_excel(file_path)\n",
    "\n",
    "# Output file for incremental saving\n",
    "output_file = 'store_forecasting_best_models.csv'\n",
    "\n",
    "# Check if the output file already exists\n",
    "if os.path.exists(output_file):\n",
    "    results_df = pd.read_csv(output_file)\n",
    "    processed_stores = set(results_df['Store Name'].values)\n",
    "else:\n",
    "    results_df = pd.DataFrame()\n",
    "    processed_stores = set()\n",
    "\n",
    "# Function to create dataset for time series\n",
    "def create_dataset(dataset, look_back=3):\n",
    "    X, Y = [], []\n",
    "    for i in range(len(dataset) - look_back):\n",
    "        X.append(dataset[i:(i + look_back), 0])\n",
    "        Y.append(dataset[i + look_back, 0])\n",
    "    return np.array(X), np.array(Y)\n",
    "\n",
    "# LSTM model\n",
    "def build_lstm(look_back):\n",
    "    model = Sequential()\n",
    "    model.add(LSTM(100, return_sequences=True, input_shape=(look_back, 1)))\n",
    "    model.add(Dropout(0.2))\n",
    "    model.add(LSTM(50))\n",
    "    model.add(Dense(1))\n",
    "    model.compile(optimizer=Adam(learning_rate=0.01), loss='mean_squared_error')\n",
    "    return model\n",
    "\n",
    "# GRU model\n",
    "def build_gru(look_back):\n",
    "    model = Sequential()\n",
    "    model.add(GRU(100, return_sequences=True, input_shape=(look_back, 1)))\n",
    "    model.add(Dropout(0.2))\n",
    "    model.add(GRU(50))\n",
    "    model.add(Dense(1))\n",
    "    model.compile(optimizer=Adam(learning_rate=0.01), loss='mean_squared_error')\n",
    "    return model\n",
    "\n",
    "# CNN-LSTM model\n",
    "def build_cnn_lstm(look_back):\n",
    "    model = Sequential()\n",
    "    model.add(Conv1D(filters=64, kernel_size=2, activation='relu', input_shape=(look_back, 1)))\n",
    "    model.add(MaxPooling1D(pool_size=2))\n",
    "    model.add(LSTM(50, return_sequences=True))\n",
    "    model.add(Dropout(0.2))\n",
    "    model.add(LSTM(25))\n",
    "    model.add(Dense(1))\n",
    "    model.compile(optimizer=Adam(learning_rate=0.01), loss='mean_squared_error')\n",
    "    return model\n",
    "\n",
    "# RandomForest and XGBoost models\n",
    "def train_tree_model(model, X_train, y_train, X_test):\n",
    "    # Remove any rows with NaN values\n",
    "    X_train, y_train = X_train.reshape(-1, look_back), y_train\n",
    "    mask = ~np.isnan(X_train).any(axis=1) & ~np.isnan(y_train)\n",
    "    X_train = X_train[mask]\n",
    "    y_train = y_train[mask]\n",
    "\n",
    "    model.fit(X_train, y_train)\n",
    "    predictions = model.predict(X_test)\n",
    "    return predictions\n",
    "\n",
    "# Hyperparameter tuning grid\n",
    "param_grid = {\n",
    "    'n_estimators': [50, 100, 200],\n",
    "    'max_depth': [None, 10, 20],\n",
    "    'min_samples_split': [2, 5]\n",
    "}\n",
    "\n",
    "# Prophet Forecasting\n",
    "def build_prophet(train_data):\n",
    "    df = pd.DataFrame({'ds': train_data.index, 'y': train_data.values})\n",
    "    model = Prophet()\n",
    "    model.fit(df)\n",
    "    future = model.make_future_dataframe(periods=len(train_data))\n",
    "    forecast = model.predict(future)\n",
    "    return forecast['yhat'].values\n",
    "\n",
    "# Iterate over each store\n",
    "for store_name in data['Store Name'].unique():\n",
    "    if store_name in processed_stores:\n",
    "        print(f\"Skipping {store_name}, already processed.\")\n",
    "        continue\n",
    "\n",
    "    # Extract data for the current store\n",
    "    store_data = data[data['Store Name'] == store_name].iloc[0, 1:].values\n",
    "    store_series = pd.Series(store_data, index=pd.date_range(start='2023-01-01', periods=len(store_data), freq='M'))\n",
    "\n",
    "    # Train-test split (80% train, 20% test)\n",
    "    train_size = int(len(store_series) * 0.8)\n",
    "    train_data = store_series[:train_size]\n",
    "    test_data = store_series[train_size:]\n",
    "\n",
    "    # Rescale the data\n",
    "    scaler = MinMaxScaler(feature_range=(0, 1))\n",
    "    train_scaled = scaler.fit_transform(train_data.values.reshape(-1, 1))\n",
    "\n",
    "    look_back = 3\n",
    "    X_train, y_train = create_dataset(train_scaled, look_back)\n",
    "    X_train = np.reshape(X_train, (X_train.shape[0], X_train.shape[1], 1))\n",
    "\n",
    "    # Prepare test data\n",
    "    test_scaled = scaler.transform(test_data.values.reshape(-1, 1))\n",
    "    X_test, y_test = create_dataset(test_scaled, look_back)\n",
    "    X_test = np.reshape(X_test, (X_test.shape[0], X_test.shape[1], 1))\n",
    "\n",
    "    # Dictionary to store metrics for each model\n",
    "    metrics = {}\n",
    "\n",
    "    # Test LSTM model\n",
    "    lstm_model = build_lstm(look_back)\n",
    "    lstm_model.fit(X_train, y_train, epochs=50, batch_size=5, verbose=0)\n",
    "    lstm_predictions = scaler.inverse_transform(lstm_model.predict(X_test))\n",
    "    lstm_mape = np.mean(np.abs((test_data[:len(lstm_predictions)] - lstm_predictions[:, 0]) / test_data[:len(lstm_predictions)])) * 100\n",
    "    metrics['LSTM'] = lstm_mape\n",
    "\n",
    "    # Test GRU model\n",
    "    gru_model = build_gru(look_back)\n",
    "    gru_model.fit(X_train, y_train, epochs=50, batch_size=5, verbose=0)\n",
    "    gru_predictions = scaler.inverse_transform(gru_model.predict(X_test))\n",
    "    gru_mape = np.mean(np.abs((test_data[:len(gru_predictions)] - gru_predictions[:, 0]) / test_data[:len(gru_predictions)])) * 100\n",
    "    metrics['GRU'] = gru_mape\n",
    "\n",
    "    # Test CNN-LSTM model\n",
    "    cnn_lstm_model = build_cnn_lstm(look_back)\n",
    "    cnn_lstm_model.fit(X_train, y_train, epochs=50, batch_size=5, verbose=0)\n",
    "    cnn_lstm_predictions = scaler.inverse_transform(cnn_lstm_model.predict(X_test))\n",
    "    cnn_lstm_mape = np.mean(np.abs((test_data[:len(cnn_lstm_predictions)] - cnn_lstm_predictions[:, 0]) / test_data[:len(cnn_lstm_predictions)])) * 100\n",
    "    metrics['CNN-LSTM'] = cnn_lstm_mape\n",
    "\n",
    "    # Test RandomForest model\n",
    "    rf_model = RandomForestRegressor()\n",
    "    # Adjust number of splits based on available data\n",
    "    n_splits = min(3, len(X_train) - look_back)  # Ensure n_splits is not greater than available samples\n",
    "    if n_splits > 1:\n",
    "        grid_search = GridSearchCV(rf_model, param_grid, scoring='neg_mean_absolute_percentage_error', cv=TimeSeriesSplit(n_splits=n_splits))\n",
    "        # Reshape and drop NaNs for RandomForest training\n",
    "        X_train_reshaped = X_train.reshape(-1, look_back)\n",
    "        mask_rf = ~np.isnan(X_train_reshaped).any(axis=1) & ~np.isnan(y_train)\n",
    "        X_train_rf = X_train_reshaped[mask_rf]\n",
    "        y_train_rf = y_train[mask_rf]\n",
    "\n",
    "        if len(X_train_rf) > 0 and len(y_train_rf) > 0:\n",
    "            grid_search.fit(X_train_rf, y_train_rf)\n",
    "            rf_predictions = grid_search.predict(X_test.reshape(-1, look_back))\n",
    "            rf_mape = mape(y_test, rf_predictions) * 100\n",
    "            metrics['RandomForest'] = rf_mape\n",
    "        else:\n",
    "            print(f\"Skipping RandomForest for {store_name} due to insufficient data.\")\n",
    "    else:\n",
    "        print(f\"Skipping RandomForest for {store_name} due to insufficient data for cross-validation.\")\n",
    "\n",
    "    # Test XGBoost model\n",
    "    xgb_model = XGBRegressor()\n",
    "    if n_splits > 1:\n",
    "        grid_search_xgb = GridSearchCV(xgb_model, param_grid, scoring='neg_mean_absolute_percentage_error', cv=TimeSeriesSplit(n_splits=n_splits))\n",
    "        # Reshape and drop NaNs for XGBoost training\n",
    "        mask_xgb = ~np.isnan(X_train_reshaped).any(axis=1) & ~np.isnan(y_train)\n",
    "        X_train_xgb = X_train_reshaped[mask_xgb]\n",
    "        y_train_xgb = y_train[mask_xgb]\n",
    "\n",
    "        if len(X_train_xgb) > 0 and len(y_train_xgb) > 0:\n",
    "            grid_search_xgb.fit(X_train_xgb, y_train_xgb)\n",
    "            xgb_predictions = grid_search_xgb.predict(X_test.reshape(-1, look_back))\n",
    "            xgb_mape = mape(y_test, xgb_predictions) * 100\n",
    "            metrics['XGBoost'] = xgb_mape\n",
    "        else:\n",
    "            print(f\"Skipping XGBoost for {store_name} due to insufficient data.\")\n",
    "    else:\n",
    "        print(f\"Skipping XGBoost for {store_name} due to insufficient data for cross-validation.\")\n",
    "\n",
    "    # Test Prophet model\n",
    "    prophet_predictions = build_prophet(train_data)\n",
    "    prophet_mape = np.mean(np.abs((test_data.values - prophet_predictions[-len(test_data):]) / test_data.values)) * 100\n",
    "    metrics['Prophet'] = prophet_mape\n",
    "\n",
    "    # Choose the best model based on the lowest MAPE\n",
    "    best_model = min(metrics, key=metrics.get)\n",
    "    best_mape = metrics[best_model]\n",
    "\n",
    "    # Store the best result for the store\n",
    "    new_result_df = pd.DataFrame([{\n",
    "        'Store Name': store_name,\n",
    "        'Best Model': best_model,\n",
    "        'MAPE (%)': best_mape\n",
    "    }])\n",
    "    \n",
    "    # Append to the main DataFrame and save\n",
    "    results_df = pd.concat([results_df, new_result_df], ignore_index=True)\n",
    "    results_df.to_csv(output_file, index=False)\n",
    "\n",
    "    print(f\"Processed and saved {store_name} with best model: {best_model} and MAPE: {best_mape}%\")\n",
    "\n",
    "# Final output\n",
    "print(\"All stores processed and saved.\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Finding average MAPE value"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "\n",
    "# Load the CSV file\n",
    "file_path = 'store_forecasting_best_models.csv'  # Make sure this is the correct path on your system\n",
    "data = pd.read_csv(file_path)\n",
    "\n",
    "# Display the first few rows to verify the data\n",
    "print(data.head())\n",
    "\n",
    "# Define a threshold for outliers\n",
    "threshold = 100  # Exclude stores with MAPE > 200%\n",
    "data_filtered = data[data['MAPE (%)'] <= threshold]\n",
    "\n",
    "# Summary statistics for MAPE without outliers\n",
    "mape_summary = data_filtered['MAPE (%)'].describe()\n",
    "print(\"\\nMAPE Summary Statistic:\\n\", mape_summary)\n",
    "\n",
    "# Calculate and print overall average MAPE without outliers\n",
    "average_mape = data_filtered['MAPE (%)'].mean()\n",
    "print(f\"\\nAverage MAPE across all stores: {average_mape:.2f}%\")\n",
    "\n",
    "# If you want to filter for stores with high MAPE (e.g., above 50%)\n",
    "high_mape_stores = data_filtered[data_filtered['MAPE (%)'] > 50]\n",
    "print(\"\\nStores with MAPE greater than 50%:\\n\", high_mape_stores)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 3. Outlier Detection and Smoothing Code"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "\n",
    "# Load the combined profits file and the best model information file\n",
    "profits_file_path = 'Cleaned_Combined_Profit_Data_2023_2024.xlsx'\n",
    "best_model_file_path = 'store_forecasting_best_models.csv'\n",
    "\n",
    "# Load the data\n",
    "profits_data = pd.read_excel(profits_file_path)\n",
    "best_model_data = pd.read_csv(best_model_file_path)\n",
    "\n",
    "# Set a threshold for high MAPE values\n",
    "high_mape_threshold = 100\n",
    "\n",
    "# Identify outlier stores based on MAPE values\n",
    "outlier_stores = best_model_data[best_model_data['MAPE (%)'] > high_mape_threshold]['Store Name'].unique()\n",
    "\n",
    "# Function to apply smoothing to the store data using a rolling average\n",
    "def smooth_data(data, window_size=3):\n",
    "    return data.rolling(window=window_size, min_periods=1).mean()\n",
    "\n",
    "# Apply smoothing and save the updated dataset for outlier stores\n",
    "for store_name in outlier_stores:\n",
    "    store_data = profits_data[profits_data['Store Name'] == store_name].iloc[0, 1:]\n",
    "    store_series = pd.Series(store_data.values, index=pd.date_range(start='2023-01-01', periods=len(store_data), freq='M'))\n",
    "    smoothed_series = smooth_data(store_series)\n",
    "    \n",
    "    # Update the profits data with smoothed values for the outlier store\n",
    "    profits_data.loc[profits_data['Store Name'] == store_name, profits_data.columns[1:]] = smoothed_series.values\n",
    "    print(f\"Applied smoothing for store: {store_name}\")\n",
    "\n",
    "# Save the smoothed dataset (optional, if you want to keep track of it separately)\n",
    "profits_data.to_excel('Smoothed_Combined_Profit_Data_2023_2024.xlsx', index=False)\n",
    "print(\"Outlier detection and smoothing complete.\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 4. Prediction Code Using Smoothed Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "import random\n",
    "from sklearn.preprocessing import MinMaxScaler\n",
    "from sklearn.ensemble import RandomForestRegressor\n",
    "from xgboost import XGBRegressor\n",
    "from prophet import Prophet\n",
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.layers import LSTM, GRU, Conv1D, MaxPooling1D, Dense, Dropout\n",
    "from tensorflow.keras.optimizers import Adam\n",
    "from tensorflow.keras.regularizers import l2\n",
    "import os\n",
    "\n",
    "# Load the smoothed profits file and the best model information file\n",
    "profits_file_path = 'Smoothed_Combined_Profit_Data_2023_2024.xlsx'\n",
    "best_model_file_path = 'store_forecasting_best_models.csv'\n",
    "\n",
    "# Load the data\n",
    "profits_data = pd.read_excel(profits_file_path)\n",
    "best_model_data = pd.read_csv(best_model_file_path)\n",
    "\n",
    "# Output file for incremental saving\n",
    "output_file = 'store_forecast_results_new.csv'\n",
    "\n",
    "# Check if the output file already exists\n",
    "if os.path.exists(output_file):\n",
    "    forecast_df = pd.read_csv(output_file)\n",
    "    processed_stores = set(forecast_df['Store Name'].unique())\n",
    "else:\n",
    "    forecast_df = pd.DataFrame()\n",
    "    processed_stores = set()\n",
    "\n",
    "# Function to create dataset\n",
    "def create_dataset(dataset, look_back=3):\n",
    "    X, Y = [], []\n",
    "    for i in range(len(dataset) - look_back):\n",
    "        X.append(dataset[i:(i + look_back), 0])\n",
    "        Y.append(dataset[i + look_back, 0])\n",
    "    return np.array(X), np.array(Y)\n",
    "\n",
    "# Function to build and compile LSTM model\n",
    "def build_lstm(look_back):\n",
    "    model = Sequential()\n",
    "    model.add(LSTM(100, return_sequences=True, input_shape=(look_back, 1), kernel_regularizer=l2(0.001)))\n",
    "    model.add(Dropout(0.2))\n",
    "    model.add(LSTM(50, kernel_regularizer=l2(0.001)))\n",
    "    model.add(Dense(1))\n",
    "    model.compile(optimizer=Adam(learning_rate=0.01), loss='mean_squared_error')\n",
    "    return model\n",
    "\n",
    "# Function to build and compile GRU model\n",
    "def build_gru(look_back):\n",
    "    model = Sequential()\n",
    "    model.add(GRU(100, return_sequences=True, input_shape=(look_back, 1), kernel_regularizer=l2(0.001)))\n",
    "    model.add(Dropout(0.2))\n",
    "    model.add(GRU(50, kernel_regularizer=l2(0.001)))\n",
    "    model.add(Dense(1))\n",
    "    model.compile(optimizer=Adam(learning_rate=0.01), loss='mean_squared_error')\n",
    "    return model\n",
    "\n",
    "# Function to build and compile CNN-LSTM model\n",
    "def build_cnn_lstm(look_back):\n",
    "    model = Sequential()\n",
    "    model.add(Conv1D(filters=64, kernel_size=2, activation='relu', input_shape=(look_back, 1)))\n",
    "    model.add(MaxPooling1D(pool_size=2))\n",
    "    model.add(LSTM(50, return_sequences=True))\n",
    "    model.add(Dropout(0.2))\n",
    "    model.add(LSTM(25))\n",
    "    model.add(Dense(1))\n",
    "    model.compile(optimizer=Adam(learning_rate=0.01), loss='mean_squared_error')\n",
    "    return model\n",
    "\n",
    "# Function to build and fit RandomForest model\n",
    "def build_random_forest(X_train, y_train):\n",
    "    model = RandomForestRegressor()\n",
    "    model.fit(X_train, y_train)\n",
    "    return model\n",
    "\n",
    "# Function to build and fit XGBoost model\n",
    "def build_xgboost(X_train, y_train):\n",
    "    model = XGBRegressor()\n",
    "    model.fit(X_train, y_train)\n",
    "    return model\n",
    "\n",
    "# Function to build and fit Prophet model\n",
    "def build_prophet(train_data):\n",
    "    df = pd.DataFrame({'ds': train_data.index, 'y': train_data.values})\n",
    "    model = Prophet()\n",
    "    model.fit(df)\n",
    "    future = model.make_future_dataframe(periods=len(train_data))\n",
    "    forecast = model.predict(future)\n",
    "    return forecast['yhat'].values\n",
    "\n",
    "# Iterate over each store to forecast using the best model\n",
    "for store_name in profits_data['Store Name'].unique():\n",
    "    if store_name in processed_stores:\n",
    "        print(f\"Skipping {store_name}, already processed.\")\n",
    "        continue\n",
    "\n",
    "    # Extract the best model for the store\n",
    "    best_model = best_model_data.loc[best_model_data['Store Name'] == store_name, 'Best Model'].values[0]\n",
    "\n",
    "    # Extract the profit data for the current store\n",
    "    store_data = profits_data[profits_data['Store Name'] == store_name].iloc[0, 1:].values\n",
    "    store_series = pd.Series(store_data, index=pd.date_range(start='2023-01-01', periods=len(store_data), freq='M'))\n",
    "\n",
    "    # Train-test split (80% train, 20% test)\n",
    "    train_size = int(len(store_series) * 0.8)\n",
    "    train_data = store_series[:train_size]\n",
    "    test_data = store_series[train_size:]\n",
    "\n",
    "    # Rescale the data for deep learning models\n",
    "    scaler = MinMaxScaler(feature_range=(0, 1))\n",
    "    train_scaled = scaler.fit_transform(train_data.values.reshape(-1, 1))\n",
    "\n",
    "    look_back = 3\n",
    "    X_train, y_train = create_dataset(train_scaled, look_back)\n",
    "    X_train = np.reshape(X_train, (X_train.shape[0], X_train.shape[1], 1))\n",
    "\n",
    "    # Prepare the test data\n",
    "    test_scaled = scaler.transform(test_data.values.reshape(-1, 1))\n",
    "    X_test, y_test = create_dataset(test_scaled, look_back)\n",
    "    X_test = np.reshape(X_test, (X_test.shape[0], X_test.shape[1], 1))\n",
    "\n",
    "    # Initialize the appropriate model based on the best model information\n",
    "    if best_model == 'LSTM':\n",
    "        model = build_lstm(look_back)\n",
    "        model.fit(X_train, y_train, epochs=50, batch_size=5, verbose=0)\n",
    "        predictions_scaled = model.predict(X_test)\n",
    "        predictions = scaler.inverse_transform(predictions_scaled).flatten()\n",
    "    \n",
    "    elif best_model == 'GRU':\n",
    "        model = build_gru(look_back)\n",
    "        model.fit(X_train, y_train, epochs=50, batch_size=5, verbose=0)\n",
    "        predictions_scaled = model.predict(X_test)\n",
    "        predictions = scaler.inverse_transform(predictions_scaled).flatten()\n",
    "\n",
    "    elif best_model == 'CNN-LSTM':\n",
    "        model = build_cnn_lstm(look_back)\n",
    "        model.fit(X_train, y_train, epochs=50, batch_size=5, verbose=0)\n",
    "        predictions_scaled = model.predict(X_test)\n",
    "        predictions = scaler.inverse_transform(predictions_scaled).flatten()\n",
    "\n",
    "    elif best_model == 'RandomForest':\n",
    "        X_train_rf, y_train_rf = create_dataset(train_data.values.reshape(-1, 1), look_back)\n",
    "        X_test_rf, y_test_rf = create_dataset(test_data.values.reshape(-1, 1), look_back)\n",
    "        model = build_random_forest(X_train_rf, y_train_rf)\n",
    "        predictions = model.predict(X_test_rf)\n",
    "\n",
    "    elif best_model == 'XGBoost':\n",
    "        X_train_xgb, y_train_xgb = create_dataset(train_data.values.reshape(-1, 1), look_back)\n",
    "        X_test_xgb, y_test_xgb = create_dataset(test_data.values.reshape(-1, 1), look_back)\n",
    "        model = build_xgboost(X_train_xgb, y_train_xgb)\n",
    "        predictions = model.predict(X_test_xgb)\n",
    "\n",
    "    elif best_model == 'Prophet':\n",
    "        predictions = build_prophet(train_data)[-len(test_data):]\n",
    "\n",
    "    else:\n",
    "        print(f\"Unknown model type for store: {store_name}\")\n",
    "        continue\n",
    "\n",
    "    # Store the forecast results in a DataFrame\n",
    "    store_forecast_df = pd.DataFrame({\n",
    "        'Store Name': [store_name] * len(predictions),\n",
    "        'Date': test_data.index[:len(predictions)],\n",
    "        'Actual': test_data.values[:len(predictions)],\n",
    "        'Predicted': predictions\n",
    "    })\n",
    "\n",
    "    # Append and save incrementally\n",
    "    forecast_df = pd.concat([forecast_df, store_forecast_df], ignore_index=True)\n",
    "    forecast_df.to_csv(output_file, index=False)\n",
    "\n",
    "    print(f\"Processed and saved {store_name}\")\n",
    "\n",
    "# Final output\n",
    "print(\"All stores processed.\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 5. Future Predictions for Each Store"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "from sklearn.preprocessing import MinMaxScaler\n",
    "from sklearn.ensemble import RandomForestRegressor\n",
    "from xgboost import XGBRegressor\n",
    "from prophet import Prophet\n",
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.layers import LSTM, GRU, Conv1D, MaxPooling1D, Dense, Dropout\n",
    "from tensorflow.keras.optimizers import Adam\n",
    "from tensorflow.keras.regularizers import l2\n",
    "import os\n",
    "\n",
    "# Load the smoothed profits file and the best model information file\n",
    "profits_file_path = 'Smoothed_Combined_Profit_Data_2023_2024.xlsx'\n",
    "best_model_file_path = 'store_forecasting_best_models.csv'\n",
    "\n",
    "# Load the data\n",
    "profits_data = pd.read_excel(profits_file_path)\n",
    "best_model_data = pd.read_csv(best_model_file_path)\n",
    "\n",
    "# Define the number of future months to predict\n",
    "future_months = 6  # For example, predicting 6 future months\n",
    "\n",
    "# Function to create dataset for LSTM/GRU/CNN-LSTM\n",
    "def create_dataset(dataset, look_back=3):\n",
    "    X, Y = [], []\n",
    "    for i in range(len(dataset) - look_back):\n",
    "        X.append(dataset[i:(i + look_back), 0])\n",
    "        Y.append(dataset[i + look_back, 0])\n",
    "    return np.array(X), np.array(Y)\n",
    "\n",
    "# LSTM model\n",
    "def build_lstm(look_back):\n",
    "    model = Sequential()\n",
    "    model.add(LSTM(100, return_sequences=True, input_shape=(look_back, 1), kernel_regularizer=l2(0.001)))\n",
    "    model.add(Dropout(0.2))\n",
    "    model.add(LSTM(50, kernel_regularizer=l2(0.001)))\n",
    "    model.add(Dense(1))\n",
    "    model.compile(optimizer=Adam(learning_rate=0.01), loss='mean_squared_error')\n",
    "    return model\n",
    "\n",
    "# GRU model\n",
    "def build_gru(look_back):\n",
    "    model = Sequential()\n",
    "    model.add(GRU(100, return_sequences=True, input_shape=(look_back, 1)))\n",
    "    model.add(Dropout(0.2))\n",
    "    model.add(GRU(50))\n",
    "    model.add(Dense(1))\n",
    "    model.compile(optimizer=Adam(learning_rate=0.01), loss='mean_squared_error')\n",
    "    return model\n",
    "\n",
    "# CNN-LSTM model\n",
    "def build_cnn_lstm(look_back):\n",
    "    model = Sequential()\n",
    "    model.add(Conv1D(filters=64, kernel_size=2, activation='relu', input_shape=(look_back, 1)))\n",
    "    model.add(MaxPooling1D(pool_size=2))\n",
    "    model.add(LSTM(50, return_sequences=True))\n",
    "    model.add(Dropout(0.2))\n",
    "    model.add(LSTM(25))\n",
    "    model.add(Dense(1))\n",
    "    model.compile(optimizer=Adam(learning_rate=0.01), loss='mean_squared_error')\n",
    "    return model\n",
    "\n",
    "# RandomForest model\n",
    "def build_random_forest(X_train, y_train):\n",
    "    model = RandomForestRegressor()\n",
    "    model.fit(X_train, y_train)\n",
    "    return model\n",
    "\n",
    "# XGBoost model\n",
    "def build_xgboost(X_train, y_train):\n",
    "    model = XGBRegressor()\n",
    "    model.fit(X_train, y_train)\n",
    "    return model\n",
    "\n",
    "# Prophet model\n",
    "def build_prophet(train_data):\n",
    "    df = pd.DataFrame({'ds': train_data.index, 'y': train_data.values})\n",
    "    model = Prophet()\n",
    "    model.fit(df)\n",
    "    future = model.make_future_dataframe(periods=future_months)\n",
    "    forecast = model.predict(future)\n",
    "    return forecast['yhat'].values\n",
    "\n",
    "# Dictionary to store future predictions for all stores\n",
    "future_predictions = {}\n",
    "\n",
    "# Iterate over each store to forecast future months using the best model\n",
    "for store_name in profits_data['Store Name'].unique():\n",
    "    # Extract the best model for the store\n",
    "    best_model = best_model_data.loc[best_model_data['Store Name'] == store_name, 'Best Model'].values[0]\n",
    "\n",
    "    # Extract the profit data for the current store\n",
    "    store_data = profits_data[profits_data['Store Name'] == store_name].iloc[0, 1:].values\n",
    "    store_series = pd.Series(store_data, index=pd.date_range(start='2023-01-01', periods=len(store_data), freq='M'))\n",
    "\n",
    "    # Rescale the data for LSTM/GRU/CNN-LSTM\n",
    "    scaler = MinMaxScaler(feature_range=(0, 1))\n",
    "    scaled_data = scaler.fit_transform(store_series.values.reshape(-1, 1))\n",
    "\n",
    "    look_back = 3\n",
    "    X, y = create_dataset(scaled_data, look_back)\n",
    "    X = np.reshape(X, (X.shape[0], X.shape[1], 1))\n",
    "\n",
    "    # Initialize the appropriate model based on the best model information\n",
    "    if best_model == 'LSTM':\n",
    "        model = build_lstm(look_back)\n",
    "        model.fit(X, y, epochs=50, batch_size=5, verbose=0)\n",
    "        is_sequence_based = True  # LSTM needs 3D sequences\n",
    "\n",
    "    elif best_model == 'GRU':\n",
    "        model = build_gru(look_back)\n",
    "        model.fit(X, y, epochs=50, batch_size=5, verbose=0)\n",
    "        is_sequence_based = True\n",
    "\n",
    "    elif best_model == 'CNN-LSTM':\n",
    "        model = build_cnn_lstm(look_back)\n",
    "        model.fit(X, y, epochs=50, batch_size=5, verbose=0)\n",
    "        is_sequence_based = True\n",
    "\n",
    "    elif best_model == 'RandomForest' or best_model == 'XGBoost':\n",
    "        X_train_rf, y_train_rf = create_dataset(store_series.values.reshape(-1, 1), look_back)\n",
    "        X_train_rf = X_train_rf.reshape(X_train_rf.shape[0], -1)  # Reshape to 2D for tree models\n",
    "        y_train_rf = y_train_rf\n",
    "        if best_model == 'RandomForest':\n",
    "            model = build_random_forest(X_train_rf, y_train_rf)\n",
    "        else:\n",
    "            model = build_xgboost(X_train_rf, y_train_rf)\n",
    "        is_sequence_based = False  # RandomForest/XGBoost needs 2D\n",
    "\n",
    "    elif best_model == 'Prophet':\n",
    "        future_store_predictions = build_prophet(store_series)[-future_months:]\n",
    "        future_predictions[store_name] = future_store_predictions\n",
    "        continue  # Skip the rest as Prophet is already complete\n",
    "\n",
    "    else:\n",
    "        print(f\"Unknown model type for store: {store_name}\")\n",
    "        continue\n",
    "\n",
    "    # Forecast future months for models except Prophet\n",
    "    last_sequence = scaled_data[-look_back:].reshape(1, look_back, 1) if is_sequence_based else scaled_data[-look_back:].reshape(1, -1)\n",
    "    future_store_predictions = []\n",
    "    for _ in range(future_months):\n",
    "        next_prediction = model.predict(last_sequence)\n",
    "\n",
    "        if is_sequence_based:\n",
    "            # For sequence-based models (LSTM, GRU, CNN-LSTM)\n",
    "            future_store_predictions.append(next_prediction[0, 0])\n",
    "            # Update the sequence with the new prediction, ensuring dimensions match\n",
    "            last_sequence = np.append(last_sequence[:, 1:, :], [[next_prediction[0]]], axis=1)\n",
    "        else:\n",
    "            # For RandomForest/XGBoost\n",
    "            future_store_predictions.append(next_prediction[0])\n",
    "            # Update the last sequence for non-sequence models\n",
    "            last_sequence = np.append(last_sequence[:, 1:], [next_prediction], axis=1)\n",
    "\n",
    "    # Inverse transform the future predictions\n",
    "    future_store_predictions = scaler.inverse_transform(np.array(future_store_predictions).reshape(-1, 1)).flatten()\n",
    "\n",
    "    # Store the future predictions in a dictionary\n",
    "    future_predictions[store_name] = future_store_predictions\n",
    "\n",
    "# Create a new DataFrame to store the predictions in the same format as the profits Excel\n",
    "future_dates = pd.date_range(start=profits_data.columns[-1], periods=future_months + 1, freq='M')[1:]\n",
    "future_df = pd.DataFrame(future_predictions, index=future_dates).T\n",
    "\n",
    "# Add the \"Store Name\" column back\n",
    "future_df.insert(0, 'Store Name', future_df.index)\n",
    "\n",
    "# Save the future predictions DataFrame\n",
    "future_output_file = 'Future_Store_Predictions.xlsx'\n",
    "future_df.to_excel(future_output_file, index=False)\n",
    "print(f\"Future predictions saved to {future_output_file}.\")\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
